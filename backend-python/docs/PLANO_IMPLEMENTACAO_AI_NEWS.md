# Plano de Implementa√ß√£o - Sistema de Not√≠cias Geradas por IA

## üìã Vis√£o Geral

Implementar um sistema completo de gera√ß√£o automatizada de not√≠cias a partir de proposi√ß√µes legislativas, transformando PDFs t√©cnicos em conte√∫do jornal√≠stico acess√≠vel ao cidad√£o comum.

## üéØ Objetivos

1. **Processar PDFs das proposi√ß√µes**: Download e extra√ß√£o de conte√∫do dos documentos legislativos
2. **Gerar not√≠cias com IA**: Criar t√≠tulo, resumo curto e mat√©ria completa usando LLMs
3. **Armazenar PDFs**: Hospedar documentos originais no Supabase Storage
4. **Persistir dados**: Salvar not√≠cias geradas no PostgreSQL
5. **Servir via API**: Endpoints para listar not√≠cias e exibir conte√∫do completo
6. **Integrar frontend**: Conectar interface existente com dados reais

---

## üèóÔ∏è Arquitetura Proposta

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FLUXO COMPLETO                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. BigQuery API (Python Backend)
   ‚îî‚îÄ> Retorna proposi√ß√µes com url_teor_proposicao

2. PDF Processing Service (Python)
   ‚îú‚îÄ> Download PDF via HTTP
   ‚îú‚îÄ> Extra√ß√£o de texto (PyPDF2/pdfplumber)
   ‚îî‚îÄ> Upload para Supabase Storage

3. AI Content Generation (LangChain/Pydantic AI)
   ‚îú‚îÄ> Processa texto extra√≠do
   ‚îú‚îÄ> Gera: t√≠tulo, resumo curto, mat√©ria completa
   ‚îî‚îÄ> Classifica impacto e relev√¢ncia

4. Database Layer (Supabase PostgreSQL via Python Backend)
   ‚îú‚îÄ> SQLAlchemy ORM para models
   ‚îú‚îÄ> Alembic para migrations
   ‚îî‚îÄ> Persiste not√≠cias geradas + metadata

5. API Layer (FastAPI - Python Backend)
   ‚îú‚îÄ> POST /news/generate - Gera not√≠cias em batch
   ‚îú‚îÄ> GET /news - Lista com t√≠tulo + resumo
   ‚îú‚îÄ> GET /news/:id - Not√≠cia completa + link PDF
   ‚îî‚îÄ> PATCH /news/:id/vote - Sistema de vota√ß√£o

6. Frontend React (Responsabilidade de outro integrante)
   ‚îú‚îÄ> Dashboard: Cards com resumo
   ‚îî‚îÄ> News Detail: Mat√©ria completa + PDF embedado
```

> **‚ö†Ô∏è IMPORTANTE**: Este plano foca no **backend Python**. A integra√ß√£o frontend ser√° feita por outro membro da equipe.

---

## üìä Modelo de Dados

### Tabela: `news` (Supabase PostgreSQL via SQLAlchemy)

```python
# backend-python/src/app/db/models/news.py

from sqlalchemy import Column, String, Text, Integer, Date, Boolean, DateTime, JSON
from sqlalchemy.dialects.postgresql import UUID
from datetime import datetime
import uuid
from app.db.schema import Base

class News(Base):
    __tablename__ = "news"

    # Primary Key
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    
    # Conte√∫do gerado pela IA
    title = Column(String(500), nullable=False, index=True)
    summary = Column(Text, nullable=False)  # 100-150 palavras
    full_content = Column(Text, nullable=False)  # 500-800 palavras
    
    # Dados da proposi√ß√£o original
    proposition_number = Column(String(20), nullable=False, index=True)  # Ex: "PL 1234/2025"
    proposition_id = Column(Integer, nullable=False, unique=True, index=True)  # id_proposicao BigQuery
    presentation_date = Column(Date, nullable=False)
    
    # Autor da proposi√ß√£o
    uf_author = Column(String(2), nullable=True, index=True)
    author_name = Column(String(100), nullable=True)
    party = Column(String(50), nullable=True)
    
    # Classifica√ß√£o
    news_type = Column(String(10), nullable=False, index=True)  # PL, PEC, EMP, etc
    original_ementa = Column(Text, nullable=False)
    
    # Armazenamento de PDF
    pdf_storage_url = Column(String(500), nullable=False)  # URL Supabase Storage
    original_pdf_url = Column(String(500), nullable=False)  # url_teor_proposicao (backup)
    
    # Engajamento e vota√ß√£o
    upvotes = Column(Integer, default=0, nullable=False)
    downvotes = Column(Integer, default=0, nullable=False)
    engagement_score = Column(Integer, default=0, nullable=False, index=True)
    
    # Publica√ß√£o em redes sociais
    published_to_social = Column(Boolean, default=False, nullable=False)
    social_publish_date = Column(DateTime, nullable=True)
    
    # Metadata adicional (tags, categorias, m√©tricas de IA)
    metadata = Column(JSON, nullable=True)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=False)
    
    def __repr__(self):
        return f"<News(id={self.id}, title='{self.title[:50]}...', proposition_id={self.proposition_id})>"
```

### Migration com Alembic

```python
# Ser√° gerada via: alembic revision --autogenerate -m "create news table"

"""create news table

Revision ID: 001
Revises: 
Create Date: 2025-11-22

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '001'
down_revision = None
branch_labels = None
depends_on = None

def upgrade():
    op.create_table('news',
        sa.Column('id', postgresql.UUID(as_uuid=True), nullable=False),
        sa.Column('title', sa.String(length=500), nullable=False),
        sa.Column('summary', sa.Text(), nullable=False),
        sa.Column('full_content', sa.Text(), nullable=False),
        sa.Column('proposition_number', sa.String(length=20), nullable=False),
        sa.Column('proposition_id', sa.Integer(), nullable=False),
        sa.Column('presentation_date', sa.Date(), nullable=False),
        sa.Column('uf_author', sa.String(length=2), nullable=True),
        sa.Column('author_name', sa.String(length=100), nullable=True),
        sa.Column('party', sa.String(length=50), nullable=True),
        sa.Column('news_type', sa.String(length=10), nullable=False),
        sa.Column('original_ementa', sa.Text(), nullable=False),
        sa.Column('pdf_storage_url', sa.String(length=500), nullable=False),
        sa.Column('original_pdf_url', sa.String(length=500), nullable=False),
        sa.Column('upvotes', sa.Integer(), nullable=False),
        sa.Column('downvotes', sa.Integer(), nullable=False),
        sa.Column('engagement_score', sa.Integer(), nullable=False),
        sa.Column('published_to_social', sa.Boolean(), nullable=False),
        sa.Column('social_publish_date', sa.DateTime(), nullable=True),
        sa.Column('metadata', postgresql.JSON(astext_type=sa.Text()), nullable=True),
        sa.Column('created_at', sa.DateTime(), nullable=False),
        sa.Column('updated_at', sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint('id')
    )
    op.create_index(op.f('ix_news_title'), 'news', ['title'], unique=False)
    op.create_index(op.f('ix_news_proposition_number'), 'news', ['proposition_number'], unique=False)
    op.create_index(op.f('ix_news_proposition_id'), 'news', ['proposition_id'], unique=True)
    op.create_index(op.f('ix_news_uf_author'), 'news', ['uf_author'], unique=False)
    op.create_index(op.f('ix_news_news_type'), 'news', ['news_type'], unique=False)
    op.create_index(op.f('ix_news_engagement_score'), 'news', ['engagement_score'], unique=False)

def downgrade():
    op.drop_index(op.f('ix_news_engagement_score'), table_name='news')
    op.drop_index(op.f('ix_news_news_type'), table_name='news')
    op.drop_index(op.f('ix_news_uf_author'), table_name='news')
    op.drop_index(op.f('ix_news_proposition_id'), table_name='news')
    op.drop_index(op.f('ix_news_proposition_number'), table_name='news')
    op.drop_index(op.f('ix_news_title'), table_name='news')
    op.drop_table('news')
```

---

## üîß Stack Tecnol√≥gica

### Backend Python (Componentes Completos)

```toml
# Adicionar ao pyproject.toml

dependencies = [
    # ... existentes ...
    "fastapi>=0.120.2",
    "sqlalchemy>=2.0.44",
    "pydantic>=2.12.3",
    
    # Database & ORM
    "alembic>=1.13.0",           # Migrations
    "psycopg2-binary>=2.9.0",    # PostgreSQL driver
    "asyncpg>=0.29.0",           # Async PostgreSQL
    
    # PDF Processing
    "pypdf2>=3.0.0",
    "pdfplumber>=0.11.0",
    
    # AI/LLM
    "langchain>=0.3.0",
    "langchain-openai>=0.2.0",
    "langchain-community>=0.3.0",
    "pydantic-ai>=1.9.0",        # J√° existe no projeto
    "openai>=2.6.1",             # J√° existe no projeto
    
    # Supabase Integration
    "supabase>=2.0.0",
    
    # HTTP Client
    "httpx>=0.27.0",
    
    # Content Processing
    "beautifulsoup4>=4.12.0",
    "markdownify>=0.12.0",
    
    # Utilities
    "python-slugify>=8.0.0",
    "python-multipart>=0.0.6",
]
```

### Configura√ß√£o Supabase

**Arquivo**: `.env` (adicionar)
```bash
# Supabase Configuration
SUPABASE_URL=https://[your-project].supabase.co
SUPABASE_KEY=[your-anon-key]
SUPABASE_SERVICE_ROLE_KEY=[your-service-role-key]
SUPABASE_BUCKET_NAME=proposition-pdfs

# Database URL para SQLAlchemy
DATABASE_URL=postgresql+asyncpg://postgres:[password]@db.[project].supabase.co:5432/postgres
```

### Supabase Setup

```sql
-- Bucket para PDFs das proposi√ß√µes
-- Criar via Dashboard Supabase > Storage > New Bucket

Bucket Name: proposition-pdfs
Public: true
File size limit: 10MB
Allowed MIME types: application/pdf

-- Storage Policy (RLS)
-- Permitir leitura p√∫blica, escrita autenticada

CREATE POLICY "Public read access"
ON storage.objects FOR SELECT
USING (bucket_id = 'proposition-pdfs');

CREATE POLICY "Authenticated upload access"
ON storage.objects FOR INSERT
WITH CHECK (bucket_id = 'proposition-pdfs');
```

---

## üìù Implementa√ß√£o Detalhada

### FASE 1: Setup e Infraestrutura (2-3 horas)

#### 1.1 Configura√ß√£o Supabase

**Passo 1**: Criar projeto no Supabase Dashboard
- URL: `https://supabase.com/dashboard`
- Copiar credenciais: URL, anon key, service role key

**Passo 2**: Criar bucket de storage
```sql
-- Via Dashboard: Storage > New Bucket
Name: proposition-pdfs
Public: true
File size limit: 10MB
```

**Passo 3**: Adicionar vari√°veis de ambiente
```bash
# .env
SUPABASE_URL=https://[project-ref].supabase.co
SUPABASE_KEY=[anon-key]
SUPABASE_SERVICE_ROLE_KEY=[service-role-key]
SUPABASE_BUCKET_NAME=proposition-pdfs
DATABASE_URL=postgresql+asyncpg://postgres:[password]@db.[project].supabase.co:5432/postgres
```

#### 1.2 Configura√ß√£o SQLAlchemy + Alembic

**Estrutura de arquivos**:
```
backend-python/
‚îú‚îÄ‚îÄ alembic.ini                    # Config Alembic
‚îú‚îÄ‚îÄ alembic/
‚îÇ   ‚îú‚îÄ‚îÄ env.py                     # Setup migrations
‚îÇ   ‚îú‚îÄ‚îÄ script.py.mako
‚îÇ   ‚îî‚îÄ‚îÄ versions/
‚îÇ       ‚îî‚îÄ‚îÄ 001_create_news_table.py
‚îú‚îÄ‚îÄ src/app/
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py              # Base declarative
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ session.py             # Database session
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ news.py            # Model News
‚îÇ   ‚îî‚îÄ‚îÄ core/
‚îÇ       ‚îî‚îÄ‚îÄ database.py            # Config conex√£o
```

**Arquivo**: `src/app/db/schema.py`
```python
from sqlalchemy.orm import declarative_base

Base = declarative_base()
```

**Arquivo**: `src/app/db/session.py`
```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from app.core.config import Config
import os

config = Config()

# Engine ass√≠ncrono
engine = create_async_engine(
    os.getenv("DATABASE_URL"),
    echo=True,  # Log SQL queries (desabilitar em produ√ß√£o)
    future=True
)

# Session factory
async_session = sessionmaker(
    engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)

async def get_db():
    """Dependency injection para FastAPI"""
    async with async_session() as session:
        try:
            yield session
        finally:
            await session.close()
```

**Arquivo**: `alembic.ini` (criar na raiz do backend-python)
```ini
[alembic]
script_location = alembic
prepend_sys_path = .
sqlalchemy.url = postgresql+asyncpg://postgres:[password]@db.[project].supabase.co:5432/postgres

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

**Arquivo**: `alembic/env.py`
```python
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context
import asyncio
import os
import sys

# Adicionar path do projeto
sys.path.insert(0, os.path.realpath(os.path.join(os.path.dirname(__file__), '..')))

from app.db.schema import Base
from app.db.models.news import News  # Importar todos os models aqui

# this is the Alembic Config object
config = context.config

# Interpret the config file for Python logging.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Usar DATABASE_URL do .env
config.set_main_option("sqlalchemy.url", os.getenv("DATABASE_URL"))

# add your model's MetaData object here
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()

def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        context.run_migrations()

async def run_async_migrations() -> None:
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()

def run_migrations_online() -> None:
    asyncio.run(run_async_migrations())

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

**Comandos Alembic**:
```bash
# Inicializar Alembic (j√° feito com estrutura acima)
cd backend-python
uv run alembic init alembic

# Gerar migration autom√°tica
uv run alembic revision --autogenerate -m "create news table"

# Aplicar migrations
uv run alembic upgrade head

# Rollback √∫ltima migration
uv run alembic downgrade -1

# Ver hist√≥rico
uv run alembic history

# Ver SQL sem aplicar
uv run alembic upgrade head --sql
```

#### 1.3 Backend-Python: Estrutura de Servi√ßos

```
backend-python/src/app/
‚îú‚îÄ‚îÄ api/v1/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ propositions.py            # J√° existe
‚îÇ   ‚îú‚îÄ‚îÄ user.py                    # J√° existe
‚îÇ   ‚îî‚îÄ‚îÄ news.py                    # NOVO - Endpoints gera√ß√£o
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ proposition_service.py     # J√° existe
‚îÇ   ‚îú‚îÄ‚îÄ user_service.py            # J√° existe
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processor_service.py   # NOVO - Download + extra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ storage_service.py         # NOVO - Upload Supabase
‚îÇ   ‚îú‚îÄ‚îÄ ai_news_generator_service.py  # NOVO - LangChain/Pydantic AI
‚îÇ   ‚îî‚îÄ‚îÄ news_orchestrator_service.py  # NOVO - Coordena fluxo
‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ news_repository.py         # NOVO - CRUD database
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ proposition.py             # J√° existe
‚îÇ   ‚îú‚îÄ‚îÄ user.py                    # J√° existe
‚îÇ   ‚îî‚îÄ‚îÄ ai_prompts.py              # NOVO - Templates prompts
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ schema.py                  # Base declarative
‚îÇ   ‚îú‚îÄ‚îÄ session.py                 # Database session
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ news.py                # NOVO - SQLAlchemy model
‚îî‚îÄ‚îÄ core/
    ‚îú‚îÄ‚îÄ config.py                  # J√° existe
    ‚îú‚îÄ‚îÄ logging.py                 # J√° existe
    ‚îî‚îÄ‚îÄ supabase_client.py         # NOVO - Cliente Supabase
```

---

### FASE 2: Servi√ßos de Processamento (4-6 horas)

#### 2.1 PDF Processor Service

**Responsabilidades**:
- Download do PDF via `url_teor_proposicao`
- Extra√ß√£o de texto estruturado
- Valida√ß√£o e limpeza do conte√∫do

**Pseudoc√≥digo**:
```python
class PDFProcessorService:
    async def download_pdf(self, url: str) -> bytes:
        """Download PDF usando httpx com retry"""
        
    async def extract_text(self, pdf_bytes: bytes) -> dict:
        """
        Retorna:
        {
            'full_text': str,
            'metadata': {
                'pages': int,
                'has_tables': bool,
                'word_count': int
            }
        }
        """
        # Tentar pdfplumber primeiro (melhor com tabelas)
        # Fallback para PyPDF2 se falhar
```

**Desafios**:
- PDFs podem estar corrompidos ‚Üí try/except robusto
- Alguns PDFs s√£o imagens escaneadas ‚Üí OCR (Tesseract) como fallback
- Limite de tamanho ‚Üí rejeitar PDFs > 10MB

#### 2.2 Storage Service (Supabase)

```python
class StorageService:
    def __init__(self):
        self.client = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        )
        self.bucket = os.getenv("SUPABASE_BUCKET_NAME")
    
    async def upload_pdf(
        self, 
        file_bytes: bytes, 
        proposition_id: int,
        filename: str
    ) -> str:
        """
        Upload e retorna URL p√∫blica
        Caminho: propositions/{ano}/{id_proposicao}/{filename}.pdf
        """
        
    async def get_public_url(self, path: str) -> str:
        """Retorna URL p√∫blica do arquivo"""
```

#### 2.3 AI News Generator Service

**Arquitetura de Prompts**:

```python
# models/ai_prompts.py

SYSTEM_PROMPT = """
Voc√™ √© um jornalista especializado em traduzir documentos legislativos 
complexos em not√≠cias acess√≠veis para o cidad√£o comum brasileiro.

Seu objetivo √©:
1. Explicar o que a proposta quer mudar na pr√°tica
2. Mostrar impactos diretos na vida das pessoas
3. Usar linguagem clara, sem jarg√µes jur√≠dicos
4. Ser imparcial mas engajador
"""

TITLE_PROMPT = """
Com base no documento legislativo abaixo, crie um t√≠tulo jornal√≠stico:

REQUISITOS:
- M√°ximo 80 caracteres
- Linguagem acess√≠vel
- Foco no impacto real (n√£o no processo legislativo)
- Tom informativo mas interessante

DOCUMENTO:
{document_text}

EMENTA OFICIAL:
{ementa}

T√çTULO:
"""

SUMMARY_PROMPT = """
Crie um resumo de 100-150 palavras respondendo:

1. O que essa proposta quer mudar?
2. Quem ser√° afetado?
3. Qual o impacto pr√°tico na vida das pessoas?

DOCUMENTO:
{document_text}

RESUMO:
"""

FULL_ARTICLE_PROMPT = """
Escreva uma mat√©ria jornal√≠stica completa (500-800 palavras) com:

ESTRUTURA:
1. Lead: Responda O QU√ä, QUEM, QUANDO, ONDE
2. Contexto: Por que essa proposta surgiu?
3. Detalhamento: Como funcionar√° na pr√°tica?
4. Impactos: Quem ganha e quem perde?
5. Pr√≥ximos passos: Tramita√ß√£o esperada

ESTILO:
- Par√°grafos curtos (3-4 linhas)
- Evite termos t√©cnicos ou explique-os
- Use exemplos concretos
- Mantenha tom neutro mas humano

DOCUMENTO:
{document_text}

AUTOR: {author_name} ({party}/{uf})
DATA: {presentation_date}
TIPO: {proposition_type}

MAT√âRIA:
"""
```

**Implementa√ß√£o do Gerador**:

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class NewsOutput(BaseModel):
    title: str = Field(max_length=80)
    summary: str = Field(min_length=100, max_length=200)
    full_content: str = Field(min_length=500, max_length=1000)
    tags: list[str] = Field(max_items=5)
    impact_level: Literal["low", "medium", "high"]
    target_audience: list[str]

class AINewsGeneratorService:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",  # Custo-benef√≠cio
            temperature=0.7
        )
    
    async def generate_news(
        self,
        pdf_text: str,
        proposition_data: dict
    ) -> NewsOutput:
        """
        Gera conte√∫do completo da not√≠cia em uma √∫nica chamada
        usando structured output do OpenAI
        """
        
        parser = PydanticOutputParser(pydantic_object=NewsOutput)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("user", self._build_full_prompt(pdf_text, proposition_data))
        ])
        
        chain = prompt | self.llm | parser
        result = await chain.ainvoke({
            "document_text": pdf_text[:8000],  # Limitar tokens
            "ementa": proposition_data["ementa"],
            "author_name": proposition_data["nome_autor"],
            "party": proposition_data["sigla_partido"],
            "uf": proposition_data["sigla_uf_autor"],
            "presentation_date": proposition_data["dataApresentacao"],
            "proposition_type": proposition_data["sigla"]
        })
        
        return result
```

**Alternativa com Pydantic AI** (mais recente):

```python
from pydantic_ai import Agent
from pydantic_ai.models.openai import OpenAIModel

class AINewsGeneratorService:
    def __init__(self):
        self.agent = Agent(
            OpenAIModel('gpt-4o-mini'),
            result_type=NewsOutput,
            system_prompt=SYSTEM_PROMPT
        )
    
    async def generate_news(
        self,
        pdf_text: str,
        proposition_data: dict
    ) -> NewsOutput:
        result = await self.agent.run(
            self._build_full_prompt(pdf_text, proposition_data)
        )
        return result.data
```

#### 2.4 News Repository (SQLAlchemy)

```python
# repositories/news_repository.py

from sqlalchemy import select, update, delete, func, or_
from sqlalchemy.ext.asyncio import AsyncSession
from app.db.models.news import News
from typing import Optional, List
from uuid import UUID

class NewsRepository:
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def create(self, news_data: dict) -> News:
        """Cria nova not√≠cia"""
        news = News(**news_data)
        self.session.add(news)
        await self.session.commit()
        await self.session.refresh(news)
        return news
    
    async def get_by_id(self, news_id: UUID) -> Optional[News]:
        """Busca not√≠cia por ID"""
        result = await self.session.execute(
            select(News).where(News.id == news_id)
        )
        return result.scalar_one_or_none()
    
    async def get_by_proposition_id(self, proposition_id: int) -> Optional[News]:
        """Busca not√≠cia por ID da proposi√ß√£o (evitar duplicatas)"""
        result = await self.session.execute(
            select(News).where(News.proposition_id == proposition_id)
        )
        return result.scalar_one_or_none()
    
    async def list_all(
        self,
        page: int = 1,
        limit: int = 20,
        uf: Optional[str] = None,
        news_type: Optional[str] = None,
        keywords: Optional[str] = None,
        order_by: str = "created_at",
        order_direction: str = "desc"
    ) -> tuple[List[News], int]:
        """Lista not√≠cias com filtros e pagina√ß√£o"""
        
        query = select(News)
        
        # Filtros
        if uf:
            query = query.where(News.uf_author == uf)
        
        if news_type:
            query = query.where(News.news_type == news_type)
        
        if keywords:
            search_pattern = f"%{keywords}%"
            query = query.where(
                or_(
                    News.title.ilike(search_pattern),
                    News.summary.ilike(search_pattern)
                )
            )
        
        # Count total
        count_query = select(func.count()).select_from(query.subquery())
        total_result = await self.session.execute(count_query)
        total = total_result.scalar()
        
        # Ordena√ß√£o
        order_column = getattr(News, order_by, News.created_at)
        if order_direction == "desc":
            query = query.order_by(order_column.desc())
        else:
            query = query.order_by(order_column.asc())
        
        # Pagina√ß√£o
        offset = (page - 1) * limit
        query = query.offset(offset).limit(limit)
        
        result = await self.session.execute(query)
        items = result.scalars().all()
        
        return items, total
    
    async def update_votes(
        self,
        news_id: UUID,
        vote_type: str
    ) -> Optional[News]:
        """Atualiza votos e recalcula engagement score"""
        news = await self.get_by_id(news_id)
        if not news:
            return None
        
        if vote_type == "upvote":
            news.upvotes += 1
        elif vote_type == "downvote":
            news.downvotes += 1
        
        news.engagement_score = news.upvotes - news.downvotes
        
        await self.session.commit()
        await self.session.refresh(news)
        return news
    
    async def mark_published_to_social(self, news_id: UUID) -> Optional[News]:
        """Marca not√≠cia como publicada nas redes sociais"""
        from datetime import datetime
        
        news = await self.get_by_id(news_id)
        if not news:
            return None
        
        news.published_to_social = True
        news.social_publish_date = datetime.utcnow()
        
        await self.session.commit()
        await self.session.refresh(news)
        return news
    
    async def get_top_engagement(self, limit: int = 10) -> List[News]:
        """Retorna not√≠cias com maior engagement score"""
        result = await self.session.execute(
            select(News)
            .order_by(News.engagement_score.desc())
            .limit(limit)
        )
        return result.scalars().all()
    
    async def delete(self, news_id: UUID) -> bool:
        """Deleta not√≠cia"""
        result = await self.session.execute(
            delete(News).where(News.id == news_id)
        )
        await self.session.commit()
        return result.rowcount > 0
```

#### 2.5 News Orchestrator Service

**Coordena todo o fluxo**:

```python
# services/news_orchestrator_service.py

from app.services.pdf_processor_service import PDFProcessorService
from app.services.storage_service import StorageService
from app.services.ai_news_generator_service import AINewsGeneratorService
from app.repositories.news_repository import NewsRepository
from app.db.session import get_db
from sqlalchemy.ext.asyncio import AsyncSession
import asyncio
import logging

logger = logging.getLogger(__name__)

class NewsOrchestratorService:
    def __init__(self, db_session: AsyncSession):
        self.pdf_processor = PDFProcessorService()
        self.storage = StorageService()
        self.ai_generator = AINewsGeneratorService()
        self.news_repo = NewsRepository(db_session)
        self.db_session = db_session
    
    async def process_proposition(
        self, 
        proposition: dict
    ) -> dict:
        """
        Pipeline completo:
        1. Verificar se j√° existe not√≠cia para esta proposi√ß√£o
        2. Download PDF
        3. Extra√ß√£o texto
        4. Upload Supabase
        5. Gera√ß√£o IA
        6. Persist√™ncia Supabase PostgreSQL
        """
        
        try:
            # 1. Verificar duplicata
            existing = await self.news_repo.get_by_proposition_id(
                proposition["id_proposicao"]
            )
            if existing:
                logger.info(f"News already exists for proposition {proposition['id_proposicao']}")
                return {
                    "success": True,
                    "news_id": str(existing.id),
                    "proposition_id": proposition["id_proposicao"],
                    "message": "Already processed"
                }
            
            # 2. Download PDF
            logger.info(f"Downloading PDF for proposition {proposition['id_proposicao']}")
            pdf_bytes = await self.pdf_processor.download_pdf(
                proposition["url_teor_proposicao"]
            )
            
            # 3. Extra√ß√£o texto
            logger.info(f"Extracting text from PDF {proposition['id_proposicao']}")
            extracted = await self.pdf_processor.extract_text(pdf_bytes)
            
            # 4. Upload Supabase Storage
            logger.info(f"Uploading PDF to Supabase {proposition['id_proposicao']}")
            pdf_url = await self.storage.upload_pdf(
                pdf_bytes,
                proposition["id_proposicao"],
                f"{proposition['sigla']}_{proposition['numero']}_{proposition['ano']}"
            )
            
            # 5. Gera√ß√£o IA
            logger.info(f"Generating news content with AI {proposition['id_proposicao']}")
            news_content = await self.ai_generator.generate_news(
                extracted["full_text"],
                proposition
            )
            
            # 6. Persistir no Supabase PostgreSQL
            from datetime import datetime
            
            news_data = {
                "title": news_content.title,
                "summary": news_content.summary,
                "full_content": news_content.full_content,
                "proposition_id": proposition["id_proposicao"],
                "proposition_number": f"{proposition['sigla']} {proposition['numero']}/{proposition['ano']}",
                "presentation_date": datetime.fromisoformat(proposition["dataApresentacao"]).date(),
                "uf_author": proposition.get("sigla_uf_autor"),
                "author_name": proposition.get("nome_autor"),
                "party": proposition.get("sigla_partido"),
                "news_type": proposition["sigla"],
                "original_ementa": proposition["ementa"] or "",
                "pdf_storage_url": pdf_url,
                "original_pdf_url": proposition["url_teor_proposicao"],
                "upvotes": 0,
                "downvotes": 0,
                "engagement_score": 0,
                "published_to_social": False,
                "metadata": {
                    "tags": news_content.tags,
                    "impact_level": news_content.impact_level,
                    "target_audience": news_content.target_audience,
                    "pdf_pages": extracted["metadata"]["pages"],
                    "word_count": extracted["metadata"]["word_count"]
                }
            }
            
            logger.info(f"Saving news to database {proposition['id_proposicao']}")
            created_news = await self.news_repo.create(news_data)
            
            return {
                "success": True,
                "news_id": str(created_news.id),
                "proposition_id": proposition["id_proposicao"],
                "title": created_news.title
            }
            
        except Exception as e:
            logger.error(f"Error processing {proposition['id_proposicao']}: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "proposition_id": proposition["id_proposicao"]
            }
    
    async def batch_process(
        self, 
        propositions: list[dict],
        max_concurrent: int = 3
    ) -> list[dict]:
        """Processa m√∫ltiplas proposi√ß√µes em paralelo"""
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_limit(prop):
            async with semaphore:
                return await self.process_proposition(prop)
        
        results = await asyncio.gather(*[
            process_with_limit(prop) for prop in propositions
        ], return_exceptions=True)
        
        # Converter exceptions em dicts
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                final_results.append({
                    "success": False,
                    "error": str(result),
                    "proposition_id": propositions[i].get("id_proposicao")
                })
            else:
                final_results.append(result)
        
        return final_results
```

---

### FASE 3: API Endpoints (2-3 horas)

#### 3.1 Backend-Python: Trigger de Gera√ß√£o

```python
# api/v1/news.py

from fastapi import APIRouter, BackgroundTasks, HTTPException, Depends
from app.services.news_orchestrator_service import NewsOrchestratorService
from app.services.proposition_service import PropositionService

router = APIRouter()

@router.post("/news/generate")
async def generate_news_from_propositions(
    background_tasks: BackgroundTasks,
    keywords: Optional[str] = None,
    uf: Optional[str] = None,
    type: Optional[str] = None,
    max_items: int = 10,
    orchestrator: NewsOrchestratorService = Depends()
):
    """
    Busca proposi√ß√µes no BigQuery e gera not√≠cias em background
    
    Response: Job ID para acompanhar progresso
    """
    
    # Buscar proposi√ß√µes
    prop_service = PropositionService()
    propositions = prop_service.list_propositions(
        keywords=keywords,
        uf=uf,
        type=type
    )[:max_items]
    
    if not propositions:
        raise HTTPException(404, "No propositions found")
    
    # Job ID para tracking
    job_id = str(uuid.uuid4())
    
    # Processar em background
    background_tasks.add_task(
        orchestrator.batch_process,
        propositions,
        max_concurrent=3
    )
    
    return {
        "job_id": job_id,
        "total_propositions": len(propositions),
        "status": "processing",
        "message": "News generation started in background"
    }

@router.post("/news/generate/{proposition_id}")
async def generate_single_news(
    proposition_id: int,
    orchestrator: NewsOrchestratorService = Depends()
):
    """Gera not√≠cia para uma proposi√ß√£o espec√≠fica (s√≠ncrono)"""
    
    # Buscar proposi√ß√£o no BigQuery
    prop_service = PropositionService()
    propositions = prop_service.list_propositions()
    
    proposition = next(
        (p for p in propositions if p["id_proposicao"] == proposition_id),
        None
    )
    
    if not proposition:
        raise HTTPException(404, f"Proposition {proposition_id} not found")
    
    result = await orchestrator.process_proposition(proposition)
    
    if not result["success"]:
        raise HTTPException(500, result["error"])
    
    return result
```

#### 3.2 Backend-Node: CRUD de Not√≠cias

```typescript
// news/news.controller.ts

@Controller('news')
export class NewsController {
  constructor(private readonly newsService: NewsService) {}

  @Get()
  async listNews(
    @Query() paginationDto: ListPaginationDto,
    @Query('uf') uf?: string,
    @Query('type') type?: string,
    @Query('keywords') keywords?: string,
  ) {
    return this.newsService.findAll({
      ...paginationDto,
      uf,
      type,
      keywords,
    });
  }

  @Get(':id')
  async getNewsDetail(@Param('id') id: string) {
    const news = await this.newsService.findOne(id);
    if (!news) {
      throw new NotFoundException(`News with ID ${id} not found`);
    }
    return news;
  }

  @Post()
  async createNews(@Body() createNewsDto: CreateNewsDto) {
    return this.newsService.create(createNewsDto);
  }

  @Patch(':id/vote')
  async voteNews(
    @Param('id') id: string,
    @Body() voteDto: { type: 'upvote' | 'downvote' },
  ) {
    return this.newsService.updateVotes(id, voteDto.type);
  }

  @Get(':id/check-social-publish')
  async checkSocialPublish(@Param('id') id: string) {
    return this.newsService.checkAndPublishToSocial(id);
  }
}
```

```typescript
// news/news.service.ts

@Injectable()
export class NewsService {
  constructor(
    @InjectRepository(News)
    private newsRepository: Repository<News>,
  ) {}

  async findAll(filters: any): Promise<PaginatedReturn<News>> {
    const queryBuilder = this.newsRepository
      .createQueryBuilder('news')
      .orderBy('news.createdAt', 'DESC');

    if (filters.uf) {
      queryBuilder.andWhere('news.ufAuthor = :uf', { uf: filters.uf });
    }

    if (filters.type) {
      queryBuilder.andWhere('news.newsType = :type', { type: filters.type });
    }

    if (filters.keywords) {
      queryBuilder.andWhere(
        '(news.title ILIKE :keywords OR news.summary ILIKE :keywords)',
        { keywords: `%${filters.keywords}%` },
      );
    }

    const [items, total] = await queryBuilder
      .skip((filters.page - 1) * filters.limit)
      .take(filters.limit)
      .getManyAndCount();

    return {
      items,
      total,
      page: filters.page,
      limit: filters.limit,
      totalPages: Math.ceil(total / filters.limit),
    };
  }

  async findOne(id: string): Promise<News> {
    return this.newsRepository.findOne({ where: { id } });
  }

  async create(createNewsDto: CreateNewsDto): Promise<News> {
    const news = this.newsRepository.create(createNewsDto);
    return this.newsRepository.save(news);
  }

  async updateVotes(id: string, voteType: 'upvote' | 'downvote'): Promise<News> {
    const news = await this.findOne(id);
    
    if (voteType === 'upvote') {
      news.upvotes++;
    } else {
      news.downvotes++;
    }

    // Calcular engagement score
    news.engagementScore = news.upvotes - news.downvotes;

    return this.newsRepository.save(news);
  }

  async checkAndPublishToSocial(id: string): Promise<{ shouldPublish: boolean }> {
    const news = await this.findOne(id);
    
    // Threshold de 100 votos positivos l√≠quidos
    const threshold = 100;
    
    if (
      news.engagementScore >= threshold &&
      !news.publishedToSocial
    ) {
      // TODO: Integrar com API de redes sociais
      news.publishedToSocial = true;
      news.socialPublishDate = new Date();
      await this.newsRepository.save(news);
      
      return { shouldPublish: true };
    }

    return { shouldPublish: false };
  }
}
```

---

### FASE 4: Documenta√ß√£o da API para Frontend (1 hora)

> **‚ö†Ô∏è IMPORTANTE**: A implementa√ß√£o frontend ser√° feita por outro integrante da equipe. Esta fase foca apenas em documentar a API.

#### 4.1 Documenta√ß√£o OpenAPI/Swagger

O FastAPI gera documenta√ß√£o autom√°tica. Ap√≥s implementa√ß√£o, estar√° dispon√≠vel em:

- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`
- **OpenAPI JSON**: `http://localhost:8000/openapi.json`

#### 4.2 Contrato da API para o Frontend

**Endpoints principais**:

| M√©todo | Endpoint | Descri√ß√£o | Response |
|--------|----------|-----------|----------|
| `GET` | `/api/v1/news` | Lista not√≠cias com filtros | `PaginatedNewsResponse` |
| `GET` | `/api/v1/news/{id}` | Detalhes de uma not√≠cia | `NewsResponse` |
| `PATCH` | `/api/v1/news/{id}/vote` | Registra voto | `NewsResponse` |
| `GET` | `/api/v1/news/top/engagement` | Top not√≠cias por engajamento | `List[NewsResponse]` |
| `POST` | `/api/v1/news/generate` | Gera not√≠cias em batch (admin) | Job ID |
| `POST` | `/api/v1/news/generate/{proposition_id}` | Gera not√≠cia √∫nica (admin) | Result dict |

#### 4.3 Atualizar Dashboard para consumir API (Frontend Team)

```typescript
// frontend/src/pages/Dashboard/index.tsx

const loadData = useCallback(async () => {
  try {
    setLoading(true);

    const params = new URLSearchParams();
    if (filters.keywords) params.append('keywords', filters.keywords);
    if (filters.uf) params.append('uf', filters.uf);
    if (filters.type) params.append('type', filters.type);
    params.append('page', '1');
    params.append('limit', '20');

    const response = await fetch(
      `${import.meta.env.VITE_API_URL}/news?${params}`
    );
    
    const data = await response.json();
    
    // Transformar para formato do frontend
    const transformedNews = data.items.map((item: any) => ({
      id: item.id,
      number: item.propositionNumber,
      presentationDate: new Date(item.presentationDate).toLocaleDateString('pt-BR'),
      description: item.summary,
      uf: item.ufAuthor,
      newsType: item.newsType.toLowerCase(),
      title: item.title,
      upvotes: item.upvotes,
      downvotes: item.downvotes,
    }));

    setNews(transformedNews);
  } catch (error) {
    console.error('Error loading data:', error);
  } finally {
    setLoading(false);
  }
}, [filters]);
```

#### 4.2 P√°gina de Not√≠cia Completa

```typescript
// frontend/src/pages/News/index.tsx

import { useParams } from 'react-router-dom';
import { PageLayout } from '@/components/Layout';
import { Badge } from '@/components/ui/badge';
import { Button } from '@/components/ui/button';
import { ThumbsUp, ThumbsDown, Share2, FileText } from 'lucide-react';
import { useState, useEffect } from 'react';

export default function News() {
  const { id } = useParams();
  const [news, setNews] = useState<any>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    const loadNews = async () => {
      try {
        const response = await fetch(
          `${import.meta.env.VITE_API_URL}/news/${id}`
        );
        const data = await response.json();
        setNews(data);
      } catch (error) {
        console.error('Error loading news:', error);
      } finally {
        setLoading(false);
      }
    };

    loadNews();
  }, [id]);

  const handleVote = async (type: 'upvote' | 'downvote') => {
    try {
      await fetch(`${import.meta.env.VITE_API_URL}/news/${id}/vote`, {
        method: 'PATCH',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ type }),
      });
      
      // Recarregar not√≠cia
      window.location.reload();
    } catch (error) {
      console.error('Error voting:', error);
    }
  };

  if (loading) return <div>Carregando...</div>;
  if (!news) return <div>Not√≠cia n√£o encontrada</div>;

  return (
    <PageLayout>
      <article className="max-w-4xl mx-auto px-4 py-8">
        {/* Header */}
        <header className="mb-8">
          <div className="flex gap-2 mb-4">
            <Badge>{news.newsType}</Badge>
            <Badge variant="outline">{news.ufAuthor}</Badge>
            <Badge variant="secondary">{news.propositionNumber}</Badge>
          </div>
          
          <h1 className="text-4xl font-bold mb-4">{news.title}</h1>
          
          <div className="flex items-center gap-4 text-muted-foreground">
            <span>Por: {news.authorName} ({news.party})</span>
            <span>‚Ä¢</span>
            <span>{new Date(news.presentationDate).toLocaleDateString('pt-BR')}</span>
          </div>
        </header>

        {/* Resumo */}
        <div className="bg-muted p-6 rounded-lg mb-8">
          <h2 className="font-semibold mb-2">Em resumo:</h2>
          <p className="text-lg leading-relaxed">{news.summary}</p>
        </div>

        {/* Conte√∫do completo */}
        <div className="prose prose-lg dark:prose-invert max-w-none mb-8">
          {news.fullContent.split('\n\n').map((paragraph: string, idx: number) => (
            <p key={idx}>{paragraph}</p>
          ))}
        </div>

        {/* PDF Embed */}
        <div className="bg-card border rounded-lg p-6 mb-8">
          <div className="flex items-center justify-between mb-4">
            <h3 className="text-xl font-semibold flex items-center gap-2">
              <FileText className="size-5" />
              Documento Original
            </h3>
            <Button variant="outline" asChild>
              <a href={news.pdfStorageUrl} target="_blank" rel="noopener noreferrer">
                Abrir PDF
              </a>
            </Button>
          </div>
          
          <iframe
            src={`${news.pdfStorageUrl}#view=FitH`}
            className="w-full h-[600px] rounded border"
            title="PDF da Proposi√ß√£o"
          />
        </div>

        {/* Vota√ß√£o e Engajamento */}
        <div className="flex items-center justify-between border-t pt-6">
          <div className="flex gap-4">
            <Button
              variant="outline"
              onClick={() => handleVote('upvote')}
              className="flex items-center gap-2"
            >
              <ThumbsUp className="size-4" />
              Relevante ({news.upvotes})
            </Button>
            
            <Button
              variant="outline"
              onClick={() => handleVote('downvote')}
              className="flex items-center gap-2"
            >
              <ThumbsDown className="size-4" />
              N√£o relevante ({news.downvotes})
            </Button>
          </div>

          <Button variant="default" className="flex items-center gap-2">
            <Share2 className="size-4" />
            Compartilhar
          </Button>
        </div>

        {/* Score de Engajamento */}
        {news.engagementScore >= 50 && (
          <div className="mt-6 bg-purple-500/10 border border-purple-500/20 rounded-lg p-4">
            <p className="text-center font-semibold">
              üî• Esta not√≠cia est√° ganhando tra√ß√£o! 
              ({news.engagementScore} votos l√≠quidos)
            </p>
            {news.publishedToSocial && (
              <p className="text-center text-sm text-muted-foreground mt-2">
                Publicado nas redes sociais em {new Date(news.socialPublishDate).toLocaleDateString('pt-BR')}
              </p>
            )}
          </div>
        )}
      </article>
    </PageLayout>
  );
}
```

---

## üöÄ Cronograma de Implementa√ß√£o

### Sprint 1: Funda√ß√µes (1 semana) - Backend Python
- [ ] Setup Supabase (bucket + credenciais + DATABASE_URL)
- [ ] Configura√ß√£o SQLAlchemy + Alembic
- [ ] Migration: Criar tabela `news`
- [ ] Model SQLAlchemy: `News`
- [ ] Repository: `NewsRepository` com CRUD completo
- [ ] Testes unit√°rios: Repository operations

### Sprint 2: Pipeline de Processamento (1 semana) - Backend Python
- [ ] PDF Processor Service completo (download + extra√ß√£o)
- [ ] Storage Service com Supabase
- [ ] AI Generator com prompts otimizados (LangChain/Pydantic AI)
- [ ] Orchestrator com tratamento de erros
- [ ] Testes: PDF download, extra√ß√£o, upload, gera√ß√£o IA

### Sprint 3: APIs e Integra√ß√£o (1 semana) - Backend Python
- [ ] Endpoints FastAPI completos (`/news/*`)
- [ ] Pydantic models para responses
- [ ] Sistema de vota√ß√£o funcional
- [ ] Check de publica√ß√£o social
- [ ] Testes E2E do pipeline completo
- [ ] Documenta√ß√£o Swagger autom√°tica

### Sprint 4: Frontend Integration (1 semana) - Frontend Team
- [ ] Dashboard com dados reais da API
- [ ] P√°gina de detalhes com PDF embed
- [ ] Sistema de vota√ß√£o UI
- [ ] Testes de usabilidade
- [ ] Responsividade mobile

### Sprint 5: Features Avan√ßadas (1 semana) - Full Team
- [ ] Publica√ß√£o autom√°tica redes sociais (Backend)
- [ ] Dashboard admin/m√©tricas (Frontend + Backend)
- [ ] Sistema de tags/categorias avan√ßado
- [ ] Otimiza√ß√µes de performance (caching, CDN)
- [ ] Monitoramento e logging (Sentry, Datadog)

---

## üìä Estimativa de Custos (OpenAI API)

### Por Not√≠cia Gerada

**Modelo**: GPT-4o-mini
- Input: ~8.000 tokens (PDF + contexto) √ó $0.150/1M = $0.0012
- Output: ~1.000 tokens (t√≠tulo + resumo + artigo) √ó $0.600/1M = $0.0006
- **Total por not√≠cia**: ~$0.002 (R$ 0.01)

**Volume estimado**:
- 1.000 not√≠cias/m√™s = $2 (R$ 10)
- 10.000 not√≠cias/m√™s = $20 (R$ 100)

**Otimiza√ß√µes**:
- Usar cache do OpenAI para PDFs similares
- Batch processing para reduzir chamadas
- Fallback para modelos open-source (Llama 3.3 via Groq)

---

## üîí Considera√ß√µes de Seguran√ßa

1. **Rate Limiting**: Limitar gera√ß√£o de not√≠cias por IP/usu√°rio
2. **Valida√ß√£o de PDFs**: Scan antiv√≠rus antes de processar
3. **Sanitiza√ß√£o de Input**: Validar URLs de PDFs (dom√≠nio camara.leg.br)
4. **Secrets Management**: Usar vari√°veis de ambiente, nunca hardcode
5. **CORS**: Configurar whitelist de origens

---

## üìà M√©tricas de Sucesso

### T√©cnicas
- Tempo m√©dio de processamento < 30s por not√≠cia
- Taxa de sucesso na extra√ß√£o de PDF > 95%
- Qualidade da not√≠cia (avalia√ß√£o manual) > 8/10

### Produto
- Engagement m√©dio (votos/visualiza√ß√µes) > 10%
- Taxa de compartilhamento > 5%
- Not√≠cias que atingem threshold de publica√ß√£o social > 2%

---

## üîÑ Pr√≥ximos Passos Ap√≥s Implementa√ß√£o

1. **Machine Learning**: 
   - Treinar modelo para classificar relev√¢ncia autom√°tica
   - Prever impacto social da proposi√ß√£o

2. **An√°lise de Sentimento**:
   - Analisar coment√°rios dos usu√°rios
   - Ajustar prompts da IA com base no feedback

3. **Notifica√ß√µes Push**:
   - Alertar usu√°rios sobre proposi√ß√µes relevantes
   - Notificar quando not√≠cia atinge threshold

4. **Integra√ß√£o Parlamentares**:
   - Dashboard para deputados verem engajamento
   - Sistema de respostas oficiais

5. **Blockchain/Web3**:
   - Registrar votos em blockchain para transpar√™ncia
   - NFTs de proposi√ß√µes com alto engajamento

---

## üìö Refer√™ncias T√©cnicas

### Documenta√ß√£o
- [LangChain Python](https://python.langchain.com/docs/introduction/)
- [Pydantic AI](https://ai.pydantic.dev/)
- [Supabase Storage](https://supabase.com/docs/guides/storage)
- [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [TypeORM Migrations](https://typeorm.io/migrations)

### Bibliotecas Chave
- `pdfplumber`: Extra√ß√£o avan√ßada de PDFs
- `langchain-openai`: Integra√ß√£o OpenAI com LangChain
- `supabase-py`: Cliente Python para Supabase
- `httpx`: HTTP client ass√≠ncrono
- `BeautifulSoup4`: Parsing HTML (se necess√°rio)

---

## ‚ùì Decis√µes Pendentes

1. **Modelo de IA**:
   - GPT-4o-mini (recomendado, custo-benef√≠cio)
   - GPT-4o (melhor qualidade, +caro)
   - Claude 3.5 Sonnet (alternativa)
   - Llama 3.3 70B via Groq (open-source, gr√°tis)

2. **Estrat√©gia de Processamento**:
   - Batch job noturno (todas as novas proposi√ß√µes)
   - On-demand (usu√°rio seleciona)
   - H√≠brido (autom√°tico + manual)

3. **Publica√ß√£o Social**:
   - Threshold fixo (100 votos)
   - Threshold din√¢mico (top 10%)
   - Aprova√ß√£o manual antes de publicar

4. **Idioma**:
   - Apenas portugu√™s
   - Suporte multil√≠ngue futuro

---

## üé® Mockups/Wireframes Sugeridos

### Dashboard - Lista de Not√≠cias
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  [Filtros: Keywords | UF | Tipo]            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ üî• Prote√ß√£o de Dados no Ambiente...   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ PL 123/2025 | SP | 01/12/2025         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Proposta estabelece novas diretrizes  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ para prote√ß√£o de dados pessoais...    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ [üëç 245] [üëé 12] [Compartilhar]        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Incentivos para Energias Renov√°veis   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ...                                    ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### P√°gina de Not√≠cia Completa
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  [PL] [SP] [PL 123/2025]                    ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  T√çTULO DA NOT√çCIA                          ‚îÇ
‚îÇ  Por: Deputado X (PARTIDO) ‚Ä¢ 01/12/2025    ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ üìå EM RESUMO:                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Texto do resumo curto...             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [Mat√©ria completa em par√°grafos]          ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ üìÑ DOCUMENTO ORIGINAL                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ [Abrir PDF]                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ [Iframe com PDF embedado]            ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [üëç Relevante (245)] [üëé N√£o relevante]   ‚îÇ
‚îÇ  [üîó Compartilhar]                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üèÅ Conclus√£o

Este plano estabelece uma arquitetura robusta e escal√°vel para transformar documentos legislativos t√©cnicos em not√≠cias acess√≠veis usando IA de ponta. 

### Responsabilidades Claras

**Backend Python (Voc√™)**:
- Pipeline completo de processamento (PDF ‚Üí IA ‚Üí Database)
- API REST com FastAPI
- Persist√™ncia com SQLAlchemy + Alembic em Supabase PostgreSQL
- Storage de PDFs em Supabase Storage
- Sistema de vota√ß√£o e engajamento
- Documenta√ß√£o OpenAPI/Swagger

**Frontend React (Outro Integrante)**:
- Consumo da API REST
- Interface de usu√°rio (Dashboard + Detalhes)
- Sistema de vota√ß√£o UI
- Integra√ß√£o com redes sociais (UI)

### Pr√≥ximo Passo Imediato

1. **Revisar este documento**
2. **Criar projeto Supabase** e configurar credenciais
3. **Ajustar prioridades** conforme necessidade da equipe
4. **Iniciar Sprint 1**: Setup de infraestrutura (SQLAlchemy + Alembic + Models)

**Tempo estimado total**: ~5 semanas (backend) + ~2 semanas (frontend) = **7 semanas para MVP completo**
